{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras image data generator is used for the generation of the batches containing the data of tensor images and is used in the domain of real-time data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the use of ImageDataGenerator\n",
    "class by passing the appropriate parameters and \n",
    "passing the required input to it. How many images\n",
    "will be generated depends on the size of the batch\n",
    "and the input data set that contains a specific number\n",
    "of inputs? For example, if the size of the batch is \n",
    "defined as 10 and we pass 1000 images in the input of outset of data then the number of images that will generate in each and every iteration of the training will be 10. The syntax that can be used for using ImageDataGenerator is as\n",
    "defined in its class of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIleuCAjoFD8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#to avoid overfitting for both test and train\n",
    "#we implement image transformation for train set\n",
    "#implement image transform only on train set to avoid overfitting\n",
    "#we can get high accuracy on train set,to avoid this we apply transformation\n",
    "#apply geometric transfromation on the images to modify them get them augumented\n",
    "#by applying this strategy our cnn relearn\n",
    "#rescale->it apply feature scaling \n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "#importing your trainset\n",
    "#reduce the sixe (64,64) to increase spead ->150,150 was slow\n",
    "#batch_size the number of images that you want in each batch\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (150, 150),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#scaling the pixel\n",
    "#we dont perform transpormation on the test set\n",
    "#test size must be the same as trian size\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (150, 150),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we are working with color images we have 64,64.3\n",
    "#if it was black and white th input was going to be 64,64,1\n",
    "#input_shpae is only mentioned for the first time when you add layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[150, 150, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimization Algorithm: Adam Optimization Algorithm is a first-order gradient-based \n",
    "optimization of stochastic function. It is a well-suited method to implement straightforwardly \n",
    "for any model in terms of large datasets and parameters. In terms of hardware resources, it \n",
    "requires less memory and is computationally very efficient. Additionally, it is well suited for \n",
    "noisy and spare gradients for non-stationary objectives and problems. Tuning is a significant \n",
    "factor for any successful model, with the adam optimization algorithm typically requiring little \n",
    "tuning [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "250/250 [==============================] - 177s 704ms/step - loss: 0.6988 - accuracy: 0.5494 - val_loss: 0.6524 - val_accuracy: 0.6010\n",
      "Epoch 2/23\n",
      "250/250 [==============================] - 139s 554ms/step - loss: 0.6501 - accuracy: 0.6201 - val_loss: 0.6365 - val_accuracy: 0.6550\n",
      "Epoch 3/23\n",
      "250/250 [==============================] - 139s 556ms/step - loss: 0.6048 - accuracy: 0.6758 - val_loss: 0.5956 - val_accuracy: 0.6840\n",
      "Epoch 4/23\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.5702 - accuracy: 0.7040 - val_loss: 0.5630 - val_accuracy: 0.7270\n",
      "Epoch 5/23\n",
      "250/250 [==============================] - 124s 498ms/step - loss: 0.5421 - accuracy: 0.7239 - val_loss: 0.5276 - val_accuracy: 0.7300\n",
      "Epoch 6/23\n",
      "250/250 [==============================] - 122s 487ms/step - loss: 0.5007 - accuracy: 0.7487 - val_loss: 0.5366 - val_accuracy: 0.7360\n",
      "Epoch 7/23\n",
      "250/250 [==============================] - 128s 510ms/step - loss: 0.4749 - accuracy: 0.7688 - val_loss: 0.5354 - val_accuracy: 0.7435\n",
      "Epoch 8/23\n",
      "250/250 [==============================] - 128s 511ms/step - loss: 0.4613 - accuracy: 0.7800 - val_loss: 0.5177 - val_accuracy: 0.7565\n",
      "Epoch 9/23\n",
      "250/250 [==============================] - 125s 501ms/step - loss: 0.4287 - accuracy: 0.7976 - val_loss: 0.5188 - val_accuracy: 0.7730\n",
      "Epoch 10/23\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.4040 - accuracy: 0.8169 - val_loss: 0.5116 - val_accuracy: 0.7700\n",
      "Epoch 11/23\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.3823 - accuracy: 0.8275 - val_loss: 0.5012 - val_accuracy: 0.7770\n",
      "Epoch 12/23\n",
      "250/250 [==============================] - 131s 525ms/step - loss: 0.3686 - accuracy: 0.8357 - val_loss: 0.5163 - val_accuracy: 0.7685\n",
      "Epoch 13/23\n",
      "250/250 [==============================] - 120s 478ms/step - loss: 0.3434 - accuracy: 0.8491 - val_loss: 0.5664 - val_accuracy: 0.7580\n",
      "Epoch 14/23\n",
      "250/250 [==============================] - 112s 446ms/step - loss: 0.3249 - accuracy: 0.8579 - val_loss: 0.5439 - val_accuracy: 0.7795\n",
      "Epoch 15/23\n",
      "250/250 [==============================] - 114s 458ms/step - loss: 0.3053 - accuracy: 0.8679 - val_loss: 0.5367 - val_accuracy: 0.7805\n",
      "Epoch 16/23\n",
      "250/250 [==============================] - 132s 526ms/step - loss: 0.2796 - accuracy: 0.8827 - val_loss: 0.5427 - val_accuracy: 0.7915\n",
      "Epoch 17/23\n",
      "250/250 [==============================] - 133s 530ms/step - loss: 0.2757 - accuracy: 0.8839 - val_loss: 0.5609 - val_accuracy: 0.7895\n",
      "Epoch 18/23\n",
      "250/250 [==============================] - 140s 559ms/step - loss: 0.2642 - accuracy: 0.8879 - val_loss: 0.5612 - val_accuracy: 0.7900\n",
      "Epoch 19/23\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 0.2330 - accuracy: 0.9006 - val_loss: 0.5828 - val_accuracy: 0.7850\n",
      "Epoch 20/23\n",
      "250/250 [==============================] - 122s 486ms/step - loss: 0.2247 - accuracy: 0.9070 - val_loss: 0.5969 - val_accuracy: 0.7800\n",
      "Epoch 21/23\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.2092 - accuracy: 0.9158 - val_loss: 0.6056 - val_accuracy: 0.7880\n",
      "Epoch 22/23\n",
      "250/250 [==============================] - 125s 499ms/step - loss: 0.1991 - accuracy: 0.9231 - val_loss: 0.6597 - val_accuracy: 0.7940\n",
      "Epoch 23/23\n",
      "250/250 [==============================] - 119s 475ms/step - loss: 0.1845 - accuracy: 0.9270 - val_loss: 0.6416 - val_accuracy: 0.7885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a0bd6b5b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = tf.keras.utils.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (150, 150))\n",
    "test_image = img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ED9KB3I54c1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Layers\n",
    "The Convolution Layers are the initial layers to \n",
    "pull out features from the image.\n",
    "It maintains the relationship between pixels\n",
    "by learning features using a small input data sequence. \n",
    "It is a mathematical term that takes two inputs,\n",
    "an image matrix and a kernel\n",
    "or filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is convolution different from pooling?\n",
    "The significant difference is that a convolution layer extracts features from the data matrix, whereas the pooling layer only downsamples the data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Pooling\n",
    "Max pooling is a rule to take the maximum of a region and help to proceed with the most crucial features from the image. It is a sample-based process that transfers continuous functions into discrete counterparts. Its primary objective is to downscale an input by reducing its dimensionality and making assumptions about features contained in the sub-region that were rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why to use Pooling Layers?\n",
    "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network.\n",
    "The pooling layer summarises the features present in a region of the feature map generated by a convolution layer. So, further operations are performed on summarised features instead of precisely positioned features generated by the convolution layer. This makes the model more robust to variations in the position of the features in the input image. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling\n",
    "The pooling layer is another building block of a CNN and plays a vital role in pre-processing an image. In the pre-process, the image size shrinks by reducing the number of parameters if the image is too large. When the picture is shrunk, the pixel density is also reduced, the downscaled image is obtained from the previous layers. Basically, its function is to progressively reduce the spatial size of the image to reduce the network complexity and computational cost. Spatial pooling is also known as downsampling or subsampling that reduces the dimensionality of each map but retains the essential features. A rectified linear activation function, or ReLU, is applied to each value in the feature map. Relu is a simple and effective nonlinearity that does not change the values in the feature map but is present because later subsequent pooling layers are added. Pooling is added after the nonlinearity is applied to the feature maps. There are three types of spatial pooling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Average Pooling\n",
    "It is different from Max Pooling; it retains information about the lesser essential features. It simply downscales by dividing the input matrix into rectangular regions and calculating the average values of each area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sum Pooling\n",
    "It is similar to Max pooling, but instead of calculating the maximum value, we calculate the mean of each sub-region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
